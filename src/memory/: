#include <arch/x86/asm_paging.h>
#include <limine.h>
#include <memory/limine_map_requests.h>
#include <memory/paging.h>

#include <stddef.h>
#include <stdint.h>

enum : size_t {
    NODE_PER_LAYER = 512, // size of pml4t, of each pdpt in each pml4, etc
    PAGE_SIZE = 4096,
    LIMINE_OFFSET = 0xffff800000000000,
};

enum FLAGS : uint64_t {
    PRESENT = 0x1,
    RW = 0x2,
    USER = 0x4,
    WRITETHROUGH = 0x8,
    CACHEDISABLE = 0x10,
    ACCESSED = 0x20,
    DIRTY = 0x40,
    SIZE = 0x80,
    XD = 0x8000000000000000,
};

// Fun fact, limine rev.3 guarantees

__attribute__((aligned(4096))) struct pml4e pml4 = {.flags = PRESENT | RW};
struct cr3_pml4_descriptor pml4r = {.flags = PRESENT | RW};

static inline size_t align_addr_start(size_t addr, size_t alignment)
{
    return addr - (addr % alignment);
}

/*
** returns a 4096k-aligned page
*/
static size_t page_idx = 0;
static void *get_zeroed_page_frame_addr(void)
{
    const size_t offs = LIMINE_OFFSET + 0x100000;

    char *addr = (char *)align_addr_start(offs + (PAGE_SIZE * page_idx), 4096);
    for (size_t i = 0; i < 4096; ++i)
    {
        addr[i] = 0;
    }
    return addr;
}

/*
** Find index at level nth PML, mark PML as present, make address point to some arbitrary cool
*higher-half address that's 4096k-aligned, go into (n-1)th PML. When PT is reached (PML0), we
*make the entry mapped to physaddr
*/
static void map_page(size_t physaddr, size_t virtualaddr, uint16_t flags)
{
    union linear_address addr = {.value = virtualaddr};

    if (!(pml4.flags & PRESENT))
    {
        void *page_add = get_zeroed_page_frame_addr();
        pml4.pdp_addr = page_add - LIMINE_OFFSET;
        ((struct pdpe *)page_add)[addr.pml4_idx] = (struct pdpe){};
        pml4.flags |= flags | PRESENT;
    }

    struct pdpe *pdpe_off = &((struct pdpe *)pml4.pdp_addr)[addr.pml4_idx];
    if (!(pdpe_off->flags & PRESENT))
    {
        pdpe_off->pd_addr = get_zeroed_page_frame_addr();
        ((struct pde *)pdpe_off->pd_addr)[addr.pdp_idx] = (struct pde){};
        pdpe_off->flags |= flags | PRESENT;
    }

    struct pde *pde_off = &((struct pde *)pdpe_off->pd_addr)[addr.pdp_idx];
    if (!(pde_off->flags & PRESENT))
    {
        pde_off->pt_addr = get_zeroed_page_frame_addr();
        ((struct pte *)pde_off->pt_addr)[addr.directory_idx] = (struct pte){};
        pde_off->flags |= flags | PRESENT;
    }

    struct pte *pte_off = &((struct pte *)pde_off->pt_addr)[addr.directory_idx];
    if (!(pde_off->flags & PRESENT))
    {
        pte_off->page_addr = physaddr;
        pte_off->flags |= flags | PRESENT;
    }
}

/* Need to remap:
 * Usable -> memmap_request
 * Bootloader reclaimable -> memmap_request
 * Framebuffer -> fb_addr from main();
 * Executable -> kern_addr_request
 * */
void paging_init(void)
{

    // Load up the kernel to its old address
    size_t kern_addr_phys = kern_add_request.response->physical_base;
    size_t kern_addr_virt = kern_add_request.response->virtual_base;
    map_page(kern_addr_phys, kern_addr_virt, PRESENT | RW);

    // Finally, we load in the new pages.
    pml4r.pml4_addr = (size_t)&pml4 - LIMINE_OFFSET;
    change_cr3(pml4r);
}
